\documentclass[11pt]{article}

\usepackage{fullpage} % Package to use full page
\usepackage{parskip} % Package to tweak paragraph skipping
\usepackage{tikz} % Package for drawing
\usepackage{amsmath}
\usepackage[breaklinks]{hyperref}
\usepackage{url}
\usepackage{breakurl} 
\usepackage{enumerate}
\usepackage{subcaption}
\usepackage{mathptmx}
\usepackage{epsfig}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{listings}

\definecolor{mGreen}{rgb}{0,0.6,0}
\definecolor{mGray}{rgb}{0.5,0.5,0.5}
\definecolor{mPurple}{rgb}{0.58,0,0.82}
\definecolor{backgroundColour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{CStyle}{
    backgroundcolor=\color{backgroundColour},   
    commentstyle=\color{mGreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{mGray},
    stringstyle=\color{mPurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    language=C
}

%\usepackage{minipage}
\def \hfillx {\hspace*{-\textwidth} \hfill}

\title{CSE 5433 OS Laboratory \\ Lab3 Report}
\author{Group 8 \\ Sixiang Ma, Xiaokuan Zhang}
\date{10/09/2017}



\begin{document}

\maketitle

\section{Objectives}
Implement the fair-share scheduling. Fair-share scheduling is to
equally distribute CPU time among users or groups in the system, as opposed to equal distribution among processes. After implementing the scheduler, we need to write kernel instrumentation code for profiling
the kernel performance in terms of the schedulerâ€™s behavior. Also, we are required to write some scripts to post-process the profiled data and plot figure(s).

\section{Scheduling Policy Design}
To implement this scheduling policy, there are several key questions we need to answer. To solve these questions, in general, there are two approaches to implement the fair-share scheduling. 
\subsection{Key Questions}
\begin{enumerate}[(a)]
\item How to implement a system call to change the scheduling policy at run-time?
\item How to know how many processes the current user has?
\item How to calculate the time slice to reflect the new policy?
\end{enumerate}

\subsection{Approach 1}
The first approach is to maintain a global flag indicating whether we should use the new policy. When the flag is set, calculate the timeslices of processes with \texttt{SCHED\_NORMAL} policy according to the number of processes the current user has. For example, if the number is N, the new timeslice would be:
\begin{equation}
\text{New Timeslice} = \frac{\text{Old Timeslice}}{N}
\end{equation}
\subsection{Approach 2}
The second approach is to implement a new policy \texttt{SCHED\_CFS}. When encounter a process with such a policy, calculate the timeslice accordingly. Also, it is required to keep track of how many \texttt{SCHED\_CFS} processes are currently running for each user.

\section{Scheduling Policy Implementation}
We implemented Approach 1, which is a lighter-weight approach compared to Approach 2. We will explain the implementation in detail and show how we solve the key questions.
\subsection{Answers to Key Questions}
\subsubsection{Adding a System Call to Set Policy}
We added a global flag, $sched\_policy\_flag$. If $sched\_policy\_flag == 1$, we schedule \texttt{SCHED\_NORMAL} processes using our new policy.
\begin{lstlisting}[style=CStyle]
int sched_policy_flag = 0;
EXPORT_SYMBOL(sched_policy_flag);
\end{lstlisting}

Moreover, we add a system call to switch the flag between 0 and 1. We also change syscall-related files accordingly. 

\begin{lstlisting}[style=CStyle]
asmlinkage long sys_sched_policy_switch(void) {
        sched_policy_flag = 1-sched_policy_flag;
        printk("SCHED FLAG: %d\n",sched_policy_flag);
        return 0;
}
\end{lstlisting}

\subsubsection{Finding the Number of Processes}
Suppose current process is $*p$. We utilized $task\_struct$ and $user\_struct$. We read (p$\rightarrow$user$\rightarrow$processes).counter (denoted $N$) to get the process count for the current user. However, this counter covers all the processes, including two irrelevant process: $sshd$ and $bash$. So we need to use $N-2$ to get the true value.

\subsubsection{Calculating the New Timeslice}
We modified the timeslice calculation mechanism to reflect the new policy. If 1) the flag is set; 2) the user id is in the test range (601 to 605); 3) it is a \texttt{SCHED\_NORMAL} process, we update the timeslice using our new scheme.
\begin{lstlisting}[style=CStyle]
static unsigned int task_timeslice(task_t *p)
{
    int N;
        if (p->static_prio < NICE_TO_PRIO(0))
                return SCALE_PRIO(DEF_TIMESLICE*4, p->static_prio);
        else
                if (sched_policy_flag == 0)
                    return SCALE_PRIO(DEF_TIMESLICE, p->static_prio);
                else
                {
                    if (((int)(p->user->uid) > 600) && ((int)(p->user->uid) < 606)) 
                    //test range
                    {
                        N = (p->user->processes).counter;
                        if (N < 3) N = 3;
                        
                        return SCALE_PRIO(DEF_TIMESLICE/(N-2), p->static_prio);
                    }
                    else
                        return SCALE_PRIO(DEF_TIMESLICE, p->static_prio);
                }
}
\end{lstlisting}

\section{Kernel Profiling}

\subsection{Profile CPU Time in the Kernel}
Now that the new scheduling policy has been implemented, we need to check its correctness in that CPU time can be equally distributed among users in the system. To this end, we have to profile the kernel performance in terms of how CPU time is assigned to processes among a group of users.  

Specifically, in $kernel/sched.c$, we declare a new struct called $struct$ $pid\_cpu\_stat$ to store the accumulative CPU time that a process has used during a period of time. Also, we declare and define another struct called $struct$ $uid\_cpu\_stat$ $uid\_stats$ to collect all the $pid\_cpu\_stat$ belonging to each system user. 

\begin{lstlisting}[style=CStyle]
#define STAT_ARRAY_MAX 10

struct pid_cpu_stat {
        int pid;
        unsigned long accum_run_time;
};
   
struct uid_cpu_stat {
        unsigned int uid;
        struct pid_cpu_stat pid_stats[STAT_ARRAY_MAX];
};

struct uid_cpu_stat uid_stats[STAT_ARRAY_MAX];
EXPORT_SYMBOL(uid_stats);
\end{lstlisting}

When a process is going to be scheduled out, we add the local variable $run\_time$ in the $schedule()$ function to the $accum\_run\_time$ variable of the corresponding $pid\_cpu\_stat$ which this process belongs to. The $run\_time$ varaible  stores the difference between the time a process is scheduled in and scheduled out. 

\subsection{Manipulate Profiling by a System Call}

To facilitate users to start and stop/dump $struct$ $uid\_cpu\_stat$ $uid\_stats$ in the kernel, we add a new system call named $sys\_sched\_profile\_switch$. Base on this system call, it is easy for users to manipulate the profiling functionality as shown in the following lines of code:

\begin{lstlisting}[style=CStyle]
int main() {
        sched_profile_switch(); 
        return 0;
}
\end{lstlisting}

By invoking $sched\_profile\_switch()$, the kernel cleans up the data for storing accumulative CPU time and starts to profile. By invoking $sched\_profile\_switch()$ again, the kernel dumps the statistics to stdout as follows and stops profiling.

\begin{lstlisting}[style=CStyle]
uid = 601
    pid = 3439, accum_run_time = 11503000
    pid = 3440, accum_run_time = 11500000

uid = 602
    pid = 3443, accum_run_time = 7611000
    pid = 3442, accum_run_time = 7590000
    pid = 3441, accum_run_time = 7590000
\end{lstlisting}

\section{Evaluation}

To evaluate the correctness of the new scheduling policy, we lauchned different numbers of processes (3-6) for UserA, UserB, UserC, UserD and performed performance profiling about 60 seconds. The results are shown in Table~\ref{tab:result}. We can observe that CPU time is equally distributed among users. In addition, for a given user, CPU time is equally distributed among its processes. These results proved that the new scheduling policy works correctly as expected. 

\begin{table}[htbp]
\centering
\begin{tabular}{ |c|c|c|c|c|c|c|c| }
\hline
& Process 1 & Process 2 & Process 3 & Process 4 & Process 5 & Process 6 & Sum \\ \hline
User A & 8.33 & 8.33 & 8.33 & N/A & N/A & N/A & 24.99 \\ \hline
User B & 6.35 & 6.35 & 6.35 & 6.32 & N/A & N/A & 25.37 \\ \hline
User C & 5.05 & 5.08 & 5.08 & 5.08 & 5.08 & N/A & 25.37 \\ \hline
User D & 4.06 & 4.04 & 4.04 & 4.04 & 4.04 & 4.04 & 24.26 \\ \hline
\end{tabular}
\caption{CPU time percentage (\%) of processes for each user.}
\vspace{-.15in}
\label{tab:result}
\end{table}

%\newpage
%\bibliographystyle{plain}
\bibliography{Group8}
\end{document}







